{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhqXxpamFARq"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi # Check whether your system has a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                       Version\n",
            "----------------------------- -----------\n",
            "absl-py                       1.4.0\n",
            "aiohttp                       3.8.4\n",
            "aiosignal                     1.3.1\n",
            "albumentations                1.3.1\n",
            "altair                        5.0.1\n",
            "asttokens                     2.2.1\n",
            "async-timeout                 4.0.2\n",
            "attrs                         23.1.0\n",
            "backcall                      0.2.0\n",
            "backports.functools-lru-cache 1.6.4\n",
            "beautifulsoup4                4.12.2\n",
            "blinker                       1.6.2\n",
            "blis                          0.7.9\n",
            "blurhash                      1.1.4\n",
            "boltons                       23.0.0\n",
            "boost                         0.1\n",
            "brotlipy                      0.7.0\n",
            "cachetools                    5.3.1\n",
            "catalogue                     2.0.8\n",
            "certifi                       2023.5.7\n",
            "cffi                          1.15.1\n",
            "charset-normalizer            2.0.4\n",
            "click                         8.1.4\n",
            "clock                         0.1\n",
            "cmake                         3.26.4\n",
            "colorama                      0.4.6\n",
            "conda                         23.5.0\n",
            "conda-content-trust           0.1.3\n",
            "conda-package-handling        2.0.2\n",
            "conda_package_streaming       0.7.0\n",
            "confection                    0.0.4\n",
            "contourpy                     1.0.7\n",
            "cryptography                  38.0.4\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.35\n",
            "de-core-news-sm               3.5.0\n",
            "debugpy                       1.5.1\n",
            "decorator                     5.1.1\n",
            "einops                        0.6.1\n",
            "en-core-web-sm                3.5.0\n",
            "et-xmlfile                    1.1.0\n",
            "executing                     1.2.0\n",
            "fastjsonschema                2.16.3\n",
            "filelock                      3.12.0\n",
            "fonttools                     4.39.0\n",
            "frozenlist                    1.3.3\n",
            "gdown                         4.6.0\n",
            "google-auth                   2.22.0\n",
            "google-auth-oauthlib          1.0.0\n",
            "GPUtil                        1.4.0\n",
            "greenlet                      2.0.2\n",
            "grpcio                        1.56.0\n",
            "idna                          3.4\n",
            "imageio                       2.31.1\n",
            "importlib-metadata            6.0.0\n",
            "imutils                       0.5.4\n",
            "ipykernel                     6.15.0\n",
            "ipython                       8.11.0\n",
            "ipywidgets                    8.0.4\n",
            "jedi                          0.18.2\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.2.0\n",
            "jsonpatch                     1.32\n",
            "jsonpointer                   2.1\n",
            "jsonschema                    4.17.3\n",
            "jupyter_client                8.0.3\n",
            "jupyter_core                  5.2.0\n",
            "jupyterlab-widgets            3.0.5\n",
            "kiwisolver                    1.4.4\n",
            "langcodes                     3.3.0\n",
            "lazy_loader                   0.2\n",
            "mahotas                       1.4.13\n",
            "Markdown                      3.4.3\n",
            "MarkupSafe                    2.1.2\n",
            "Mastodon.py                   1.8.1\n",
            "matplotlib                    3.7.1\n",
            "matplotlib-inline             0.1.6\n",
            "menuinst                      1.4.19\n",
            "mpmath                        1.3.0\n",
            "multidict                     6.0.4\n",
            "murmurhash                    1.0.9\n",
            "nbformat                      5.8.0\n",
            "nest-asyncio                  1.5.6\n",
            "networkx                      3.1\n",
            "numpy                         1.24.2\n",
            "oauthlib                      3.2.2\n",
            "openai                        0.27.7\n",
            "opencv-python                 4.7.0.72\n",
            "opencv-python-headless        4.8.0.74\n",
            "openface                      0.0.0\n",
            "openpyxl                      3.1.2\n",
            "packaging                     23.0\n",
            "pandas                        1.5.3\n",
            "parso                         0.8.3\n",
            "pathy                         0.10.1\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        9.4.0\n",
            "pip                           22.3.1\n",
            "platformdirs                  3.1.0\n",
            "plotly                        5.14.0\n",
            "pluggy                        1.0.0\n",
            "portalocker                   2.7.0\n",
            "preshed                       3.0.8\n",
            "prompt-toolkit                3.0.38\n",
            "protobuf                      4.23.4\n",
            "psutil                        5.9.0\n",
            "pure-eval                     0.2.2\n",
            "pyasn1                        0.5.0\n",
            "pyasn1-modules                0.3.0\n",
            "pycosat                       0.6.4\n",
            "pycparser                     2.21\n",
            "pydantic                      1.10.8\n",
            "Pygments                      2.14.0\n",
            "PyJWT                         2.7.0\n",
            "pyOpenSSL                     22.0.0\n",
            "pyparsing                     3.0.9\n",
            "pyproject                     1.3.1\n",
            "pyrsistent                    0.19.3\n",
            "PySocks                       1.7.1\n",
            "python-dateutil               2.8.2\n",
            "python-magic-bin              0.4.14\n",
            "pytz                          2022.7.1\n",
            "pytz-deprecation-shim         0.1.0.post0\n",
            "pyu2f                         0.1.5\n",
            "PyWavelets                    1.4.1\n",
            "pywin32                       305.1\n",
            "PyYAML                        5.1\n",
            "pyzmq                         23.2.0\n",
            "qudida                        0.0.4\n",
            "requests                      2.28.1\n",
            "requests-oauthlib             1.3.1\n",
            "rsa                           4.9\n",
            "ruamel.yaml                   0.17.21\n",
            "ruamel.yaml.clib              0.2.6\n",
            "scikit-image                  0.21.0\n",
            "scikit-learn                  1.2.2\n",
            "scipy                         1.10.1\n",
            "seaborn                       0.12.2\n",
            "setuptools                    65.6.3\n",
            "six                           1.16.0\n",
            "sklearn                       0.0.post1\n",
            "smart-open                    6.3.0\n",
            "soupsieve                     2.4.1\n",
            "spacy                         3.5.3\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.4\n",
            "SQLAlchemy                    2.0.18\n",
            "srsly                         2.4.6\n",
            "stack-data                    0.6.2\n",
            "sympy                         1.11.1\n",
            "tenacity                      8.2.2\n",
            "tensorboard                   2.13.0\n",
            "tensorboard-data-server       0.7.1\n",
            "texttable                     1.6.7\n",
            "thinc                         8.1.10\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2023.4.12\n",
            "toolz                         0.12.0\n",
            "torch                         2.0.1\n",
            "torchdata                     0.6.1\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.2\n",
            "torchvision                   0.15.2\n",
            "tornado                       6.2\n",
            "tqdm                          4.64.1\n",
            "traitlets                     5.9.0\n",
            "typer                         0.7.0\n",
            "typing_extensions             4.4.0\n",
            "tzdata                        2023.3\n",
            "tzlocal                       4.3\n",
            "unzip                         1.0.0\n",
            "urllib3                       1.26.14\n",
            "wasabi                        1.1.1\n",
            "wcwidth                       0.2.6\n",
            "Werkzeug                      2.3.6\n",
            "wheel                         0.37.1\n",
            "widgetsnbextension            4.0.5\n",
            "win-inet-pton                 1.1.0\n",
            "wincertstore                  0.2\n",
            "yarl                          1.9.2\n",
            "zipp                          3.15.0\n",
            "zstandard                     0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ru1dLP8vXUnVbDiwQqf--qyiYkHd9C_1\n",
            "To: d:\\SRM-IEEE-Winter-School\\SRM-IEEE-SPS-Winter-School\\07_12_22\\flower_photos.tgz\n",
            "\n",
            "  0%|          | 0.00/229M [00:00<?, ?B/s]\n",
            "  0%|          | 524k/229M [00:00<00:54, 4.19MB/s]\n",
            "  1%|          | 1.57M/229M [00:00<00:37, 6.10MB/s]\n",
            "  1%|▏         | 3.15M/229M [00:00<00:32, 6.88MB/s]\n",
            "  2%|▏         | 5.24M/229M [00:00<00:22, 9.77MB/s]\n",
            "  3%|▎         | 6.29M/229M [00:00<00:23, 9.63MB/s]\n",
            "  3%|▎         | 7.86M/229M [00:00<00:21, 10.2MB/s]\n",
            "  4%|▍         | 9.44M/229M [00:01<00:20, 10.5MB/s]\n",
            "  5%|▍         | 11.0M/229M [00:01<00:20, 10.6MB/s]\n",
            "  5%|▌         | 12.6M/229M [00:01<00:19, 11.1MB/s]\n",
            "  6%|▌         | 14.2M/229M [00:01<00:22, 9.43MB/s]\n",
            "  7%|▋         | 16.3M/229M [00:01<00:19, 11.0MB/s]\n",
            "  8%|▊         | 17.8M/229M [00:01<00:18, 11.1MB/s]\n",
            "  8%|▊         | 19.4M/229M [00:01<00:19, 10.9MB/s]\n",
            "  9%|▉         | 21.0M/229M [00:02<00:19, 10.9MB/s]\n",
            " 10%|▉         | 22.5M/229M [00:02<00:18, 11.2MB/s]\n",
            " 11%|█         | 24.1M/229M [00:02<00:19, 10.4MB/s]\n",
            " 11%|█         | 25.7M/229M [00:02<00:20, 9.89MB/s]\n",
            " 12%|█▏        | 27.3M/229M [00:02<00:18, 10.6MB/s]\n",
            " 13%|█▎        | 28.8M/229M [00:02<00:20, 9.71MB/s]\n",
            " 13%|█▎        | 30.4M/229M [00:03<00:19, 10.2MB/s]\n",
            " 14%|█▍        | 32.0M/229M [00:03<00:18, 10.5MB/s]\n",
            " 15%|█▍        | 33.6M/229M [00:03<00:19, 9.98MB/s]\n",
            " 15%|█▌        | 35.1M/229M [00:03<00:18, 10.5MB/s]\n",
            " 16%|█▌        | 36.7M/229M [00:03<00:17, 10.8MB/s]\n",
            " 17%|█▋        | 38.3M/229M [00:03<00:17, 10.8MB/s]\n",
            " 17%|█▋        | 39.8M/229M [00:03<00:17, 10.8MB/s]\n",
            " 18%|█▊        | 41.4M/229M [00:04<00:16, 11.1MB/s]\n",
            " 19%|█▉        | 43.0M/229M [00:04<00:16, 11.2MB/s]\n",
            " 19%|█▉        | 44.6M/229M [00:04<00:16, 10.9MB/s]\n",
            " 20%|██        | 46.1M/229M [00:04<00:16, 11.3MB/s]\n",
            " 21%|██        | 47.7M/229M [00:04<00:15, 11.5MB/s]\n",
            " 22%|██▏       | 49.3M/229M [00:04<00:16, 11.1MB/s]\n",
            " 22%|██▏       | 50.9M/229M [00:04<00:15, 11.1MB/s]\n",
            " 23%|██▎       | 52.4M/229M [00:05<00:15, 11.0MB/s]\n",
            " 24%|██▎       | 54.0M/229M [00:05<00:20, 8.62MB/s]\n",
            " 24%|██▍       | 55.1M/229M [00:05<00:20, 8.62MB/s]\n",
            " 25%|██▍       | 56.6M/229M [00:05<00:18, 9.24MB/s]\n",
            " 25%|██▌       | 58.2M/229M [00:05<00:17, 9.89MB/s]\n",
            " 26%|██▌       | 59.8M/229M [00:05<00:16, 10.2MB/s]\n",
            " 27%|██▋       | 61.3M/229M [00:05<00:15, 10.6MB/s]\n",
            " 27%|██▋       | 62.9M/229M [00:06<00:15, 11.0MB/s]\n",
            " 28%|██▊       | 64.5M/229M [00:06<00:15, 10.8MB/s]\n",
            " 29%|██▉       | 66.1M/229M [00:06<00:14, 11.0MB/s]\n",
            " 30%|██▉       | 67.6M/229M [00:06<00:14, 11.3MB/s]\n",
            " 30%|███       | 69.2M/229M [00:06<00:14, 11.2MB/s]\n",
            " 31%|███       | 70.8M/229M [00:06<00:13, 11.5MB/s]\n",
            " 32%|███▏      | 72.4M/229M [00:06<00:13, 11.5MB/s]\n",
            " 32%|███▏      | 73.9M/229M [00:07<00:13, 11.3MB/s]\n",
            " 33%|███▎      | 75.5M/229M [00:07<00:13, 11.7MB/s]\n",
            " 34%|███▎      | 77.1M/229M [00:07<00:12, 11.8MB/s]\n",
            " 34%|███▍      | 78.6M/229M [00:07<00:13, 11.3MB/s]\n",
            " 35%|███▌      | 80.2M/229M [00:07<00:12, 11.5MB/s]\n",
            " 36%|███▌      | 81.8M/229M [00:07<00:13, 11.1MB/s]\n",
            " 36%|███▋      | 83.4M/229M [00:07<00:12, 11.6MB/s]\n",
            " 37%|███▋      | 84.9M/229M [00:08<00:12, 11.5MB/s]\n",
            " 38%|███▊      | 86.5M/229M [00:08<00:12, 11.1MB/s]\n",
            " 38%|███▊      | 88.1M/229M [00:08<00:13, 10.4MB/s]\n",
            " 39%|███▉      | 89.7M/229M [00:08<00:12, 11.1MB/s]\n",
            " 40%|███▉      | 91.2M/229M [00:08<00:12, 11.2MB/s]\n",
            " 41%|████      | 92.8M/229M [00:08<00:11, 11.6MB/s]\n",
            " 41%|████      | 94.4M/229M [00:08<00:11, 11.4MB/s]\n",
            " 42%|████▏     | 95.9M/229M [00:09<00:11, 11.6MB/s]\n",
            " 43%|████▎     | 97.5M/229M [00:09<00:11, 11.5MB/s]\n",
            " 43%|████▎     | 99.1M/229M [00:09<00:11, 11.4MB/s]\n",
            " 44%|████▍     | 101M/229M [00:09<00:11, 11.3MB/s] \n",
            " 45%|████▍     | 102M/229M [00:09<00:11, 11.3MB/s]\n",
            " 45%|████▌     | 104M/229M [00:09<00:11, 11.3MB/s]\n",
            " 46%|████▌     | 105M/229M [00:09<00:10, 11.3MB/s]\n",
            " 47%|████▋     | 107M/229M [00:10<00:11, 10.5MB/s]\n",
            " 47%|████▋     | 109M/229M [00:10<00:11, 10.5MB/s]\n",
            " 48%|████▊     | 110M/229M [00:10<00:10, 10.9MB/s]\n",
            " 49%|████▉     | 112M/229M [00:10<00:13, 8.84MB/s]\n",
            " 49%|████▉     | 113M/229M [00:10<00:12, 9.40MB/s]\n",
            " 50%|█████     | 115M/229M [00:10<00:11, 10.1MB/s]\n",
            " 51%|█████     | 116M/229M [00:10<00:10, 10.6MB/s]\n",
            " 52%|█████▏    | 118M/229M [00:11<00:10, 10.9MB/s]\n",
            " 52%|█████▏    | 120M/229M [00:11<00:09, 11.3MB/s]\n",
            " 53%|█████▎    | 121M/229M [00:11<00:09, 10.9MB/s]\n",
            " 54%|█████▎    | 123M/229M [00:11<00:09, 11.5MB/s]\n",
            " 54%|█████▍    | 124M/229M [00:11<00:10, 10.2MB/s]\n",
            " 55%|█████▍    | 126M/229M [00:11<00:09, 10.5MB/s]\n",
            " 56%|█████▌    | 127M/229M [00:11<00:09, 10.8MB/s]\n",
            " 56%|█████▋    | 129M/229M [00:12<00:10, 9.36MB/s]\n",
            " 57%|█████▋    | 131M/229M [00:12<00:09, 9.90MB/s]\n",
            " 58%|█████▊    | 132M/229M [00:12<00:09, 10.2MB/s]\n",
            " 58%|█████▊    | 134M/229M [00:12<00:09, 10.3MB/s]\n",
            " 59%|█████▉    | 135M/229M [00:12<00:08, 10.6MB/s]\n",
            " 60%|█████▉    | 137M/229M [00:12<00:09, 9.43MB/s]\n",
            " 60%|██████    | 138M/229M [00:13<00:09, 9.82MB/s]\n",
            " 61%|██████    | 139M/229M [00:13<00:09, 9.55MB/s]\n",
            " 62%|██████▏   | 141M/229M [00:13<00:08, 10.0MB/s]\n",
            " 62%|██████▏   | 143M/229M [00:13<00:08, 10.6MB/s]\n",
            " 63%|██████▎   | 144M/229M [00:13<00:07, 10.9MB/s]\n",
            " 64%|██████▎   | 146M/229M [00:13<00:07, 11.1MB/s]\n",
            " 64%|██████▍   | 147M/229M [00:13<00:07, 10.8MB/s]\n",
            " 65%|██████▌   | 149M/229M [00:14<00:07, 11.0MB/s]\n",
            " 66%|██████▌   | 150M/229M [00:14<00:06, 11.3MB/s]\n",
            " 66%|██████▋   | 152M/229M [00:14<00:06, 11.2MB/s]\n",
            " 67%|██████▋   | 154M/229M [00:14<00:06, 11.6MB/s]\n",
            " 68%|██████▊   | 155M/229M [00:14<00:06, 11.1MB/s]\n",
            " 69%|██████▊   | 157M/229M [00:14<00:06, 11.2MB/s]\n",
            " 69%|██████▉   | 158M/229M [00:14<00:06, 11.4MB/s]\n",
            " 70%|██████▉   | 160M/229M [00:15<00:05, 11.5MB/s]\n",
            " 71%|███████   | 161M/229M [00:15<00:05, 11.5MB/s]\n",
            " 71%|███████▏  | 163M/229M [00:15<00:05, 11.5MB/s]\n",
            " 72%|███████▏  | 165M/229M [00:15<00:05, 11.7MB/s]\n",
            " 73%|███████▎  | 166M/229M [00:15<00:05, 11.6MB/s]\n",
            " 73%|███████▎  | 168M/229M [00:15<00:06, 8.98MB/s]\n",
            " 74%|███████▍  | 169M/229M [00:15<00:06, 9.61MB/s]\n",
            " 74%|███████▍  | 170M/229M [00:16<00:06, 9.69MB/s]\n",
            " 75%|███████▌  | 172M/229M [00:16<00:05, 10.2MB/s]\n",
            " 76%|███████▌  | 174M/229M [00:16<00:05, 10.7MB/s]\n",
            " 77%|███████▋  | 175M/229M [00:16<00:04, 11.0MB/s]\n",
            " 77%|███████▋  | 177M/229M [00:16<00:04, 11.0MB/s]\n",
            " 78%|███████▊  | 178M/229M [00:16<00:05, 9.95MB/s]\n",
            " 79%|███████▉  | 180M/229M [00:16<00:04, 11.6MB/s]\n",
            " 80%|███████▉  | 182M/229M [00:17<00:04, 11.5MB/s]\n",
            " 80%|████████  | 184M/229M [00:17<00:03, 11.3MB/s]\n",
            " 81%|████████  | 185M/229M [00:17<00:03, 11.3MB/s]\n",
            " 82%|████████▏ | 187M/229M [00:17<00:03, 11.6MB/s]\n",
            " 82%|████████▏ | 188M/229M [00:17<00:03, 11.6MB/s]\n",
            " 83%|████████▎ | 190M/229M [00:17<00:03, 11.1MB/s]\n",
            " 84%|████████▎ | 191M/229M [00:17<00:03, 11.2MB/s]\n",
            " 84%|████████▍ | 193M/229M [00:18<00:03, 11.4MB/s]\n",
            " 85%|████████▌ | 195M/229M [00:18<00:02, 11.5MB/s]\n",
            " 86%|████████▌ | 196M/229M [00:18<00:03, 10.0MB/s]\n",
            " 87%|████████▋ | 198M/229M [00:18<00:02, 11.8MB/s]\n",
            " 87%|████████▋ | 200M/229M [00:18<00:02, 11.7MB/s]\n",
            " 88%|████████▊ | 201M/229M [00:18<00:02, 11.7MB/s]\n",
            " 89%|████████▊ | 203M/229M [00:18<00:02, 11.7MB/s]\n",
            " 89%|████████▉ | 204M/229M [00:19<00:02, 11.6MB/s]\n",
            " 90%|█████████ | 206M/229M [00:19<00:02, 11.2MB/s]\n",
            " 91%|█████████ | 208M/229M [00:19<00:01, 11.4MB/s]\n",
            " 91%|█████████▏| 209M/229M [00:19<00:01, 11.7MB/s]\n",
            " 92%|█████████▏| 211M/229M [00:19<00:01, 11.6MB/s]\n",
            " 93%|█████████▎| 212M/229M [00:19<00:01, 11.7MB/s]\n",
            " 93%|█████████▎| 214M/229M [00:19<00:01, 11.7MB/s]\n",
            " 94%|█████████▍| 215M/229M [00:20<00:01, 11.8MB/s]\n",
            " 95%|█████████▍| 217M/229M [00:20<00:01, 11.7MB/s]\n",
            " 96%|█████████▌| 219M/229M [00:20<00:00, 11.1MB/s]\n",
            " 96%|█████████▌| 220M/229M [00:20<00:00, 11.2MB/s]\n",
            " 97%|█████████▋| 222M/229M [00:20<00:00, 11.1MB/s]\n",
            " 98%|█████████▊| 223M/229M [00:20<00:00, 11.6MB/s]\n",
            " 98%|█████████▊| 225M/229M [00:20<00:00, 11.5MB/s]\n",
            " 99%|█████████▉| 226M/229M [00:21<00:00, 8.98MB/s]\n",
            " 99%|█████████▉| 228M/229M [00:21<00:00, 8.96MB/s]\n",
            "100%|██████████| 229M/229M [00:21<00:00, 9.51MB/s]\n",
            "100%|██████████| 229M/229M [00:21<00:00, 10.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_url = \"https://drive.google.com/file/d/1Ru1dLP8vXUnVbDiwQqf--qyiYkHd9C_1/view?usp=share_link\"\n",
        "!gdown 1Ru1dLP8vXUnVbDiwQqf--qyiYkHd9C_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.unpack_archive('flower_photos.tgz','./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf  flower_photos/LICENSE.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "names = os.listdir('flower_photos')\n",
        "os.makedirs(os.path.join('flower_photos/train/'),exist_ok=True)\n",
        "for name in names:\n",
        "    shutil.move(f'flower_photos/{name}',f'flower_photos/train/')\n",
        "    os.makedirs(os.path.join('flower_photos/validation/',name),exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "train_names = os.listdir('flower_photos/train/')\n",
        "for train_name in train_names:\n",
        "    file_names = sorted(os.listdir(os.path.join('flower_photos/train/',train_name)))\n",
        "    for file_name in file_names[-100:]:\n",
        "        shutil.move(os.path.join('flower_photos/train/',train_name,file_name),os.path.join('flower_photos/validation/',train_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTN7o73CxpU0"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9NoduTJIqEvv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch \n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ycU-vBtaCv"
      },
      "source": [
        "numpy \n",
        "- A scientific computing library\n",
        "- Supports array operations and manipulations\n",
        "- Built-in functions like array sum, diff, mean, average, min, max etc.\n",
        "\n",
        "cv2\n",
        "- To read and write images\n",
        "- To perform basic vision algorithms like RGB to HSV, Gaussian Blur, Dilation, Histogram, Shape detection etc.\n",
        "\n",
        "torch\n",
        "- The open source ML framework for deep learning\n",
        "- Has a pythonic interface\n",
        "\n",
        "torchvision\n",
        "- consists of popular datasets, model architectures, and common image transformations for computer vision\n",
        "- Contains built in data augmentations\n",
        "\n",
        "matplotlib\n",
        "- It is a data visualization library for python\n",
        "- It supports graphical plotting and viewing images and other data\n",
        "\n",
        "nn\n",
        "- The base module with which we can create and train neural nets\n",
        "- Provides built-in classes for common neural network layers like Conv, Pooling, Activations, Normalizations, Dropout etc.\n",
        "\n",
        "optim\n",
        "- It is a package for implementing various optimization algorithms for model training\n",
        "- Built-in optimizers like SGD, Adam, Adagrad, RMSprop etc.\n",
        "\n",
        "tqdm\n",
        "- It is a library used to display a progress bar for loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Qbwuzdx0dH"
      },
      "source": [
        "# Preprocess inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rVcCk-6TyvEy"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()]) # Define the set of transformations to be applied on the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lFD-xj8qWVc"
      },
      "outputs": [],
      "source": [
        "# transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeJCeo3yE9Z"
      },
      "source": [
        "ToTensor()\n",
        " - converts the image with a pixel range of [0, 255] to a PyTorch FloatTensor of shape (C, H, W) with a range [0.0, 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uzgtjA4xCnv"
      },
      "source": [
        "Normalize() \n",
        "- output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "\n",
        "- Normalization helps get data within a range and reduces the skewness which helps learn faster and better. \n",
        "- Normalization can also tackle the diminishing and exploding gradients problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dZes3DQzlsm"
      },
      "source": [
        "# Custom Dataloaders using Image Folder function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_oL6gEw01fO"
      },
      "source": [
        "- Combines a dataset and a sampler\n",
        "- Provides an iterable over the given dataset\n",
        "- Supports single- or multi-process loading\n",
        "- Customizing loading order\n",
        "- Automatic batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset = datasets.ImageFolder(root ='flower_photos/train',transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v-258nSWz5i9"
      },
      "outputs": [],
      "source": [
        "valset = datasets.ImageFolder(root ='flower_photos/validation',transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nDZYc7_xz-tM"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(trainloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cxx5qFSCMdl"
      },
      "source": [
        "Now that we have an iterator, let's visualize our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J9Qg2ipB0Acv"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images, gt_labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataiter) \u001b[39m# The first minibatch of data\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[1;32mc:\\Users\\SNU\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "images, gt_labels = next(dataiter) # The first minibatch of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVoe0YEL0Dtu"
      },
      "outputs": [],
      "source": [
        "print(images.shape)\n",
        "print(gt_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTOnzN15uaN8"
      },
      "outputs": [],
      "source": [
        " # Visualize an image sample\n",
        "plt.imshow((images[2].squeeze()).permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = trainset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBvtMO96ud6w"
      },
      "outputs": [],
      "source": [
        "# Visualize all images in the minibatch\n",
        "figure = plt.figure()\n",
        "total_samples = 8\n",
        "for index in range(1,total_samples+1):\n",
        "    ax = plt.subplot(2, 4, index)\n",
        "    ax.set_xlabel(classes[gt_labels[index-1].item()])\n",
        "    # plt.axis('off')\n",
        "    plt.imshow(images[index-1].squeeze().permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u1I-5qDGiQV"
      },
      "outputs": [],
      "source": [
        "# Visualize the Ground truth labels\n",
        "print(gt_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr_TSdxjGo5k"
      },
      "outputs": [],
      "source": [
        "# Visualize the onehot encoding of the labels\n",
        "onehot_labels = nn.functional.one_hot(gt_labels, num_classes=10)\n",
        "print(onehot_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's build our AlexNet\n",
        "![](assets/AlexNet.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIT4IGppv_ql"
      },
      "outputs": [],
      "source": [
        "model = AlexNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujZRzrUDu2yp"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "epochs = 100\n",
        "pbar = tqdm(range(epochs))\n",
        "best_accuracy = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_init = time() # record the time at which training started\n",
        "\n",
        "for e in pbar:\n",
        "    running_loss = 0\n",
        "    for images, gt_labels in tqdm(trainloader):\n",
        "        images = images.cuda()\n",
        "        gt_labels = gt_labels.cuda()\n",
        "    \n",
        "        # Training pass\n",
        "        optimizer.zero_grad() # reset the gradients of model weights\n",
        "        \n",
        "        output = model(images)\n",
        "        loss = criterion(output, gt_labels)\n",
        "        \n",
        "        # Calculate the gradients of the learnable parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Modify the model weights as per the gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    average_loss = running_loss/len(trainloader)\n",
        "    \n",
        "    # perform validation\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    for images,gt_labels in valloader:\n",
        "      pred_probs = model(images.cuda())\n",
        "      predicted_labels = torch.argmax(pred_probs, dim=-1)\n",
        "\n",
        "      for i in range(len(gt_labels)):\n",
        "        if predicted_labels[i]==gt_labels[i]:\n",
        "          correct_count = correct_count+1\n",
        "        total_count = total_count+1       \n",
        "\n",
        "    accuracy = correct_count/total_count\n",
        "    \n",
        "    # save the model weights\n",
        "    if accuracy>=best_accuracy:\n",
        "      best_accuracy = accuracy\n",
        "      torch.save(model, './best_flower_model.pt')\n",
        "    torch.save(model, './last_flower_model.pt') \n",
        "\n",
        "    print(f\"\\nEpoch {e} - Training loss: {average_loss}, val accuracy : {accuracy}\")\n",
        "print(f\"Training Time (in minutes) = {(time()-time_init)/60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, gt_labels = next(iter(valloader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_probs = model(images.cuda())\n",
        "\n",
        "predicted_labels = torch.argmax(pred_probs, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all images in the minibatch\n",
        "figure = plt.figure()\n",
        "total_samples = 8\n",
        "for index in range(1,total_samples+1):\n",
        "    ax = plt.subplot(2, 4, index)\n",
        "    ax.set_xlabel(classes[gt_labels[index-1].item()])\n",
        "    # plt.axis('off')\n",
        "    plt.imshow(images[index-1].squeeze().permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Ground truth labels : {gt_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Predicted labels : {predicted_labels}\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all images in the minibatch\n",
        "figure = plt.figure()\n",
        "total_samples = 8\n",
        "for index in range(1,total_samples+1):\n",
        "    ax = plt.subplot(2, 4, index)\n",
        "    ax.set_xlabel(classes[predicted_labels[index-1].item()])\n",
        "    # plt.axis('off')\n",
        "    plt.imshow(images[index-1].squeeze().permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct_count = 0\n",
        "total_count = 0\n",
        "for i in range(len(gt_labels)):\n",
        "  img = images[i]\n",
        "  if predicted_labels[i]==gt_labels[i]:\n",
        "    correct_count = correct_count+1\n",
        "  total_count = total_count+1\n",
        "accuracy = correct_count/total_count\n",
        "print(f\"The accuracy on the minibatch is : {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for images,gt_labels in valloader:\n",
        "  pred_probs = model(images.cuda())\n",
        "  predicted_labels = torch.argmax(pred_probs, dim=-1)\n",
        "\n",
        "  for i in range(len(gt_labels)):\n",
        "    if predicted_labels[i]==gt_labels[i]:\n",
        "      correct_count = correct_count+1\n",
        "    total_count = total_count+1       \n",
        "\n",
        "accuracy = correct_count/total_count\n",
        "print(f\"Number of validation images = {total_count}\\n\")\n",
        "print(f\"Model Accuracy = {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wCn3pKjc_iTF",
        "NfSyK7w6_7Ba",
        "VFOu2DDNPAju",
        "GgZgs_bPQMwg"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('torchenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0c9fb112d4624b5e4fa7b873160be5d487c9f44e6948be985e0928a4033755e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
